# 会话记录：2026-01-19

## 背景

- 当前集群宿主机系统/工具链较老，宿主机环境中无法可靠安装与运行 PyTorch（例如 `pip/conda install torch` 失败或运行不稳定）。
- 因此，深度学习训练/推理（含 CLG-ODE、VectorLSTM、GNN baseline）统一要求在容器内运行，并通过 Slurm 提交作业。

## 本次变更

### 1) 明确“禁止本地 DL 训练”的规则

- 更新 `AGENTS.md`：新增明确规则，禁止在宿主机运行/安装深度学习相关环境，必须采用容器工作流；并重申 `sbatch` 需由用户提交。

### 2) CLG-ODE：稀疏预测口径修正（实现层面）

- 统一用“带 soft mask 的期望权重”作为 SC 预测：`A_pred = p_hat * ŵ`（其中 `p_hat = sigmoid(a_logit)`，`ŵ = a_weight`）。
- 评估指标（`sc_log_*`）与训练中 `L_weight/L_topo/零边惩罚` 的输入改为 `A_pred`，避免 `softplus` 输出恒正导致的 dense 伪阳性。
- 额外加入可选的 `p_hat` 边数约束（`lambda_density`）与零边幅值惩罚的 warmup/ramp 参数（便于稳定训练）。

### 3) 文档更新：容器化训练与提交方式

- 更新 `docs/workflow.md`：在 CLG-ODE 部分强调不要在宿主机直接运行 `python -m scripts.train_clg_ode`，改为推荐 `sbatch scripts/submit_clg_ode.sh` 或 `sbatch --array=0-4 scripts/submit_clg_ode.sh`。
- 更新 `docs/cluster_gpu_usage.md`：补充“CLG-ODE 必须在容器内运行”的提醒，并指向仓库提交脚本。
- 更新 `README.md`：将“模型训练”示例改为使用 `sbatch` 提交脚本（用户提交）。

## 如何提交 CLG-ODE（用户执行）

单折（默认由脚本读取 `configs/paths.yaml` 解析路径）：

```bash
sbatch scripts/submit_clg_ode.sh
```

5 折 array：

```bash
sbatch --array=0-4 scripts/submit_clg_ode.sh
```

常用超参（通过环境变量传入脚本）：

```bash
LAMBDA_DENSITY=0.05 \
DENSITY_WARMUP_EPOCHS=10 \
DENSITY_RAMP_EPOCHS=20 \
LAMBDA_ZERO_LOG=0.05 \
ZERO_LOG_WARMUP_EPOCHS=10 \
ZERO_LOG_RAMP_EPOCHS=20 \
sbatch --array=0-4 scripts/submit_clg_ode.sh
```

## 新增：fold0 测试方案（用户执行）

用于快速验证“mask 学习 + 稀疏对齐”的一组提交脚本：

```bash
bash scripts/submit_clg_ode_mask_fold0_batch.sh
```

包含 4 个子实验：

- `submit_clg_ode_mask_fold0_a.sh`：对照（无 density/zero 约束）
- `submit_clg_ode_mask_fold0_b.sh`：仅密度/边数约束（`lambda_density` + warmup/ramp）
- `submit_clg_ode_mask_fold0_c.sh`：仅零边幅值惩罚（`lambda_zero_log` + warmup/ramp）
- `submit_clg_ode_mask_fold0_d.sh`：密度 + 零边组合

## 结果摘要（4 个任务已完成）

结果目录：

- `outputs/results/clg_ode/runs/clg_ode_mask_fold0_a_control/fold0/`
- `outputs/results/clg_ode/runs/clg_ode_mask_fold0_b_density/fold0/`
- `outputs/results/clg_ode/runs/clg_ode_mask_fold0_c_zero/fold0/`
- `outputs/results/clg_ode/runs/clg_ode_mask_fold0_d_density_zero/fold0/`

关键观察（以 `test_sc_metrics.json` 为准）：

- 4 个方案的整体相关均稳定在 `sc_log_pearson ≈ 0.863`，说明 `A_pred = p_hat * w_hat` 的口径修正后，稀疏真值上的相关不再异常偏低。
- 密度约束（B）对稀疏口径指标更敏感：`sc_log_pearson_topk=0.6003`、`ecc_l2=11257.8`（本组最优），但 density 启用后更容易触发早停（仅训练到 16 epochs）。
- 零边惩罚（C）在本组配置下对 `sc_log_mse` 更敏感（0.1234 最小），但对 top-k/sparse 指标提升不明显。

详细表格与下一步参数建议见：`docs/reports/clg_ode_mask_fold0_20260119.md`。

## 备注

- 本次未在宿主机进行任何 PyTorch 安装或训练测试；所有训练需在容器内由用户提交作业验证。
