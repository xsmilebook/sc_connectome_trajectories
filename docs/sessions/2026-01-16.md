# 2026-01-16 会话记录

## 变更摘要

- 针对当前训练损失失衡问题，更新 `PLAN.md`：明确 topo 的数据驱动缩放（q80/q90）+ `log1p` 压缩、仅对 topo/manifold 使用 GradNorm、并设置 20% cosine warmup 的策略与验证步骤。
- 更新 `PROGRESS.md` 的当前关注点为上述损失稳定化计划。
- 落地拓扑损失稳定化策略：对 `L_topo` 进行分位数尺度归一化与 `log1p` 压缩，加入 GradNorm（仅 `L_manifold/L_topo`），并对 topo 权重施加 20% cosine warmup。
- 训练入口新增参数：`--topo_scale_quantile`、`--topo_warmup_frac`、`--gradnorm_scope`、`--disable_topo_log_compress`。
- 更新 `docs/methods.md` 与 `docs/workflow.md`，记录 topo 归一化/GradNorm/warmup 与新增指标字段。
